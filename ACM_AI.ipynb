{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sancayenne06/ACMAI/blob/main/ACM_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEQw2EvAWjp5"
      },
      "source": [
        "# LLM finetuning\n",
        "\n",
        "You should make a copy of this notebook and work on it there.\n",
        "\n",
        "In this assignement we're going to use hugging face libraries to turn an LLM into a limited instruct model (all this means is that rather than make the LLM fill in the next char in a sequence it'll write based off of an instruction). This is important because we're going to be using trl to tune r1 and knowing how to use peft will make it much more efficent. The goal of this is not to test your understanding or to test if you can use chatgpt but rather to make sure that you can read hugging face's documentation and understand what you need to do to make it work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6boxLqpWiCk"
      },
      "outputs": [],
      "source": [
        "# the libraries you will need\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "from trl import SFTConfig, SFTTrainer\n",
        "from peft import LoraConfig, TaskType"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "keut_oR2aBoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoAblQE4aIBf",
        "outputId": "a853b81f-1c8c-4186-d338-75f4afdd48f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# load smolLM from hugging face (only ~100M params)\n",
        "# note you will want to set your colab runtime to use the gpu\n",
        "MODEL_NAME = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # set runtime to T4 GPU\n",
        "print(f\"Using device: {device}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk6b2hHabkz2"
      },
      "outputs": [],
      "source": [
        "# ok lets try prompting it with an instruction, note that it doesn't obey the instruction\n",
        "# and rather just repeats the format (likely because its never seen this before)\n",
        "test_prompt = f\"### Prompt: Compose a poem, about blue skys \\n ### Response:\"\n",
        "inputs = tokenizer.encode(test_prompt, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs, max_length=50, temperature=0.2, top_k=50, do_sample=True)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCH8uTj-Z3C0"
      },
      "outputs": [],
      "source": [
        "# lets load a dataset with instruction output pairs from hugging face\n",
        "dataset = load_dataset(\"checkai/instruction-poems\", split=\"train[:30%]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_rQpKZzeLBi"
      },
      "outputs": [],
      "source": [
        "# lets check out an example from the dataset\n",
        "print(\"Instruction:\\n\", dataset[\"INSTRUCTION\"][0])\n",
        "print(\"Response:\\n\", dataset[\"RESPONSE\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fywyCI_Qbm7T"
      },
      "outputs": [],
      "source": [
        "# make a peft config, good params are:\n",
        "# r = 8\n",
        "# lora_alpha = 32\n",
        "# lora_dropout = 0.05\n",
        "# make sure you know what those mean\n",
        "\n",
        "\n",
        "# now we need a function that formats our data into instruction and response\n",
        "def formatting_func(example):\n",
        "    text = f\"### Prompt: {example['INSTRUCTION']}\\n ### Response: {example['RESPONSE']}\"\n",
        "    return text\n",
        "\n",
        "# create our SFTTrainer\n",
        "# since we don't have powerful gpus I would reccomend restricting\n",
        "# max_seq_length to something like 100\n",
        "\n",
        "# train our model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDdt2VaEPum8",
        "outputId": "c504d6b9-8656-462f-9872-4013b8d87df9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Prompt: Write a poem about the sea\n",
            " ### Response: \n",
            "I love the sea. It's so beautiful. It's so peaceful. It's so calm. It's so peaceful.\n",
            " ### Prompt: Write a poem about the sea\n"
          ]
        }
      ],
      "source": [
        "# now when we give it the following prompt we expect to see some better outputs\n",
        "# ideally poetry (even if its bad), something original not in the query\n",
        "# mine output:\n",
        "\"\"\"\n",
        "### Prompt: Write a poem about the sea\n",
        " ### Response:\n",
        "I love the sea. It's so beautiful. It's so peaceful. It's so calm. It's so peaceful.\n",
        " ### Prompt: Write a poem about the sea\n",
        " \"\"\"\n",
        "\n",
        "instruction = \"Write a poem about the sea\"\n",
        "prompt = f\"### Prompt: {instruction}\\n ### Response: \"\n",
        "\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "outputs = model.generate(input_ids, max_length=50, temperature=0.2, top_k=50, do_sample=True)\n",
        "\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report\n",
        "Write a short report about what you did and why (we mostly want to check you didn't just use chatgpt for the entire thing)."
      ],
      "metadata": {
        "id": "A9INeERlbC6J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8YJx72zatrd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}